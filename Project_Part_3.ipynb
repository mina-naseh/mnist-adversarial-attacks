{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDfewngP4NCC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Project Part 3: Adversarial, Transferability and Robustification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfLSN2Lr1spr"
   },
   "source": [
    "We recommand you to use Google Colab to edit and run this notebook. You can also install jupyter on your own computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6TZpU6QwAYp6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmdwipDFgh90"
   },
   "source": [
    "## 0. Prepare data\n",
    "\n",
    "You can familiarise yourself with MNIST, a small size dataset, on its Wikipedia article [https://en.wikipedia.org/wiki/MNIST_database](https://en.wikipedia.org/wiki/MNIST_database). MNIST is composed of 28x28 grayscaled images of handwritten digits. This is a classification task with 10 classes (10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IcmjJHdxBDuZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HdRM7qC3KBiI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = mnist[\"data\"]\n",
    "y = mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wi4EdVlaBy5-",
    "outputId": "03a1c422-0d9a-45b7-90c2-d32a225c10ea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (70000, 784)\n",
      "Min, max x: (0, 255)\n",
      "Shape of y: (70000,)\n",
      "Classes in y: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9']\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "print(f\"Shape of x: {x.shape}\")\n",
    "print(f\"Min, max x: {x.min(), x.max()}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "print(f\"Classes in y: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GCUKJb8fCXvx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Preprocessing\n",
    "# # x = torch.from_numpy(x.astype(float)).float()\n",
    "# # y = torch.from_numpy(y.astype(int)).type(torch.LongTensor)\n",
    "# x = torch.tensor(x, dtype=torch.float32)\n",
    "# y = torch.tensor(y.astype(int), dtype=torch.long)\n",
    "\n",
    "# # Shape\n",
    "# x = x.reshape(-1, 1, 28, 28)\n",
    "# # Scaler\n",
    "# x = (x - x.min()) / (x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (70000, 784)\n",
      "Min, max x: (0, 255)\n",
      "Shape of y: (70000,)\n",
      "Classes in y: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Loads and prepares the MNIST dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - test_size (float): Proportion of the dataset to include in the test split.\n",
    "    - val_size (float): Proportion of the train dataset to include in the validation split.\n",
    "    - random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - x_train, x_val, x_test (np.ndarray): Training, validation, and testing feature sets.\n",
    "    - y_train, y_val, y_test (np.ndarray): Corresponding labels.\n",
    "    \"\"\"\n",
    "    # Load the MNIST dataset\n",
    "    mnist = fetch_openml('mnist_784', as_frame=False, cache=True)\n",
    "    x = mnist[\"data\"]\n",
    "    y = mnist[\"target\"]\n",
    "    \n",
    "    # Data exploration\n",
    "    print(f\"Shape of x: {x.shape}\")\n",
    "    print(f\"Min, max x: {x.min(), x.max()}\")\n",
    "    print(f\"Shape of y: {y.shape}\")\n",
    "    print(f\"Classes in y: {np.unique(y)}\")\n",
    "    \n",
    "    # Train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=test_size, random_state=random_state, stratify=y, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Train-validation split\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train, y_train, test_size=val_size, random_state=random_state, stratify=y_train, shuffle=True\n",
    "    )\n",
    "    \n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "# Prepare data\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tensor shape: torch.Size([44800, 1, 28, 28])\n",
      "y_train_tensor shape: torch.Size([44800])\n"
     ]
    }
   ],
   "source": [
    "def convert_to_tensor(data, dtype=torch.float32, reshape=None, normalize=False):\n",
    "    \"\"\"\n",
    "    Convert a NumPy array to a PyTorch tensor with optional reshaping and normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (np.ndarray): Input NumPy array.\n",
    "    - dtype (torch.dtype): Desired PyTorch data type.\n",
    "    - reshape (tuple): New shape for the tensor (optional).\n",
    "    - normalize (bool): Whether to normalize data to [0, 1].\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: Converted PyTorch tensor.\n",
    "    \"\"\"\n",
    "    tensor = torch.tensor(data, dtype=dtype)\n",
    "    if reshape:\n",
    "        tensor = tensor.reshape(reshape)\n",
    "    if normalize:\n",
    "        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    return tensor\n",
    "\n",
    "def prepare_tensors(x_train, x_val, x_test, y_train, y_val, y_test):\n",
    "    \"\"\"\n",
    "    Prepares PyTorch tensors for the dataset by reshaping and normalizing.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train, x_val, x_test (np.ndarray): Feature datasets.\n",
    "    - y_train, y_val, y_test (np.ndarray): Label datasets.\n",
    "\n",
    "    Returns:\n",
    "    - Tensors for training, validation, and testing datasets (features and labels).\n",
    "    \"\"\"\n",
    "    x_train_tensor = convert_to_tensor(x_train, dtype=torch.float32, reshape=(-1, 1, 28, 28), normalize=True)\n",
    "    x_val_tensor = convert_to_tensor(x_val, dtype=torch.float32, reshape=(-1, 1, 28, 28), normalize=True)\n",
    "    x_test_tensor = convert_to_tensor(x_test, dtype=torch.float32, reshape=(-1, 1, 28, 28), normalize=True)\n",
    "\n",
    "    y_train_tensor = convert_to_tensor(y_train.astype(int), dtype=torch.long)\n",
    "    y_val_tensor = convert_to_tensor(y_val.astype(int), dtype=torch.long)\n",
    "    y_test_tensor = convert_to_tensor(y_test.astype(int), dtype=torch.long)\n",
    "    \n",
    "    return x_train_tensor, x_val_tensor, x_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor\n",
    "\n",
    "# Prepare tensors\n",
    "x_train_tensor, x_val_tensor, x_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor = prepare_tensors(\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test\n",
    ")\n",
    "\n",
    "# Verify the results\n",
    "print(f\"x_train_tensor shape: {x_train_tensor.shape}\")\n",
    "print(f\"y_train_tensor shape: {y_train_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkcToEG85CCC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Adversarial examples\n",
    "\n",
    "The goal of this first part is to generate adversarial examples on a simple dataset called MNIST. MNIST is a dataset of 28x28 black and white images that represents hand-written digits, and their associate label 0,1,...,9.\n",
    "\n",
    "You can use the following ressource to help you [https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9unhURg05qcR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Train a Neural Network using the PyTorch library.\n",
    "\n",
    "The architecture of the models and the training hyper-parameters are given below.\n",
    "We recommend using these parameters, the SGD optimizer and the Cross Entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OAsZAMTvMKPX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gTG7NSpuiI6r"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.01  # Slightly increase learning rate for faster convergence\n",
    "# momentum = 0.9  # Keep momentum unchanged\n",
    "# epochs = 2  # Reduce epochs to quickly validate the code\n",
    "# batch_size = 8  # Reduce batch size to fit in limited memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0 = Net()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=learning_rate, momentum=momentum)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0  # Track total loss for the epoch\n",
    "    \n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader), total=int(size / batch_size), desc=\"Training\"):\n",
    "        # Move data and labels to the appropriate device\n",
    "        X, y = X.to(next(model.parameters()).device), y.to(next(model.parameters()).device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        ## YOUR CODE HERE:\n",
    "        preds = model(X)  # Forward pass\n",
    "        loss = loss_fn(preds, y)  # Compute the loss\n",
    "\n",
    "        # Backpropagation\n",
    "        ## YOUR CODE HERE:\n",
    "        optimizer.zero_grad()  # Reset gradients to zero\n",
    "        loss.backward()  # Backward pass: compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Track the loss for this batch\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Return the average loss for the epoch\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9lwTxQ60kaXd"
   },
   "outputs": [],
   "source": [
    "# # ## GIVEN, to evaluate the progress of the training at each epoch\n",
    "# def val_loop(dataloader, model, loss_fn, epoch_i):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss, correct = 0, 0\n",
    "\n",
    "#     # Set model to evaluation mode\n",
    "#     model.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             # Move data and labels to the same device as the model\n",
    "#             X, y = X.to(next(model.parameters()).device), y.to(next(model.parameters()).device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             pred = model(X)\n",
    "\n",
    "#             # Compute loss and accuracy\n",
    "#             test_loss += loss_fn(pred, y).item()\n",
    "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "#     # Compute average loss and accuracy\n",
    "#     test_loss /= num_batches\n",
    "#     correct /= size\n",
    "\n",
    "#     # Print metrics\n",
    "#     print(f\"Epoch {epoch_i}, Val Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn, epoch_i, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "    - model (torch.nn.Module): Model to evaluate.\n",
    "    - loss_fn (torch.nn.Module): Loss function.\n",
    "    - epoch_i (int): Current epoch number.\n",
    "    - verbose (bool): Whether to print validation metrics.\n",
    "\n",
    "    Returns:\n",
    "    - float: Average validation loss.\n",
    "    - float: Validation accuracy (percentage).\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not isinstance(dataloader, DataLoader):\n",
    "        raise ValueError(\"dataloader must be an instance of torch.utils.data.DataLoader\")\n",
    "\n",
    "    # Initialize evaluation\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss, num_correct = 0.0, 0\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # Evaluate in batches\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(X)\n",
    "\n",
    "            # Compute loss and accuracy\n",
    "            val_loss += loss_fn(preds, y).item()\n",
    "            num_correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    val_loss /= num_batches\n",
    "    accuracy = (num_correct / total_samples) * 100\n",
    "\n",
    "    # Print metrics if verbose\n",
    "    if verbose:\n",
    "        print(f\"Epoch {epoch_i}, Val Error: Accuracy: {accuracy:>0.1f}%, Avg loss: {val_loss:>8f}\")\n",
    "\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hn6HM3Y7HP6E",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def train_model(model, x_train, y_train, x_val, y_val, optimizer, batch_size, loss_func, epochs):\n",
    "#     # Data processing\n",
    "#     train_dataset = TensorDataset(x_train, y_train)\n",
    "#     train_loader = DataLoader(\n",
    "#         dataset=train_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True,\n",
    "#         num_workers=2,\n",
    "#     )\n",
    "#     val_dataset = TensorDataset(x_val, y_val)\n",
    "#     val_loader = DataLoader(\n",
    "#         dataset=val_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         num_workers=2,\n",
    "#     )\n",
    "\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         print(f\"Epoch {epoch}/{epochs}\")\n",
    "#         model.train()  # Set model to training mode\n",
    "\n",
    "#         # Training loop\n",
    "#         train_loss = 0.0\n",
    "#         for X, y in tqdm(train_loader, desc=\"Training\"):\n",
    "#             # Move data to the same device as the model\n",
    "#             X, y = X.to(next(model.parameters()).device), y.to(next(model.parameters()).device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             preds = model(X)\n",
    "#             loss = loss_func(preds, y)\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#         # Average train loss for this epoch\n",
    "#         avg_train_loss = train_loss / len(train_loader)\n",
    "#         print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "#         # Validation loop\n",
    "#         model.eval()  # Set model to evaluation mode\n",
    "#         val_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for X, y in val_loader:\n",
    "#                 X, y = X.to(next(model.parameters()).device), y.to(next(model.parameters()).device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 preds = model(X)\n",
    "#                 loss = loss_func(preds, y)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#                 # Compute accuracy\n",
    "#                 pred_classes = preds.argmax(dim=1)\n",
    "#                 correct += (pred_classes == y).sum().item()\n",
    "#                 total += y.size(0)\n",
    "\n",
    "#         # Average validation loss and accuracy\n",
    "#         avg_val_loss = val_loss / len(val_loader)\n",
    "#         val_accuracy = correct / total * 100\n",
    "#         print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, optimizer, batch_size, loss_func, epochs):\n",
    "    \"\"\"\n",
    "    Train the model and evaluate on validation data after each epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model to train.\n",
    "    - x_train, y_train (torch.Tensor): Training features and labels.\n",
    "    - x_val, y_val (torch.Tensor): Validation features and labels.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for parameter updates.\n",
    "    - batch_size (int): Batch size for training.\n",
    "    - loss_func (torch.nn.Module): Loss function.\n",
    "    - epochs (int): Number of epochs to train.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing training and validation metrics.\n",
    "    \"\"\"\n",
    "    # Prepare data loaders\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Track metrics\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "    # Training and validation for each epoch\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        # Training loop\n",
    "        train_loss = train_loop(train_loader, model, loss_func, optimizer, batch_size)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss, val_accuracy = val_loop(val_loader, model, loss_func, epoch_i=epoch, verbose=False)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Log metrics\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zDzi2wGpk-nw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:26<00:00, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4926, Validation Accuracy: 87.29%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:28<00:00, 24.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3245, Validation Accuracy: 91.46%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:23<00:00, 29.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2671, Validation Accuracy: 92.67%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:38<00:00, 18.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2198, Validation Accuracy: 93.71%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:30<00:00, 22.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1887, Validation Accuracy: 94.52%\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:24<00:00, 28.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1702, Validation Accuracy: 94.97%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:18<00:00, 37.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1552, Validation Accuracy: 95.34%\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:19<00:00, 36.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1468, Validation Accuracy: 95.80%\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:14<00:00, 49.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1368, Validation Accuracy: 95.78%\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [00:13<00:00, 53.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1265, Validation Accuracy: 96.24%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [1.2921131033556803,\n",
       "  0.702658565725599,\n",
       "  0.5554901785935674,\n",
       "  0.4844007088669709,\n",
       "  0.4331578298977443,\n",
       "  0.3966901205480099,\n",
       "  0.3643378542363644,\n",
       "  0.34076620885304043,\n",
       "  0.32424190448863166,\n",
       "  0.31129711951528277],\n",
       " 'val_loss': [0.4926051558767046,\n",
       "  0.32453633499997003,\n",
       "  0.26705293084893905,\n",
       "  0.21976818195411138,\n",
       "  0.188719513629164,\n",
       "  0.17018323852547576,\n",
       "  0.15517446294426918,\n",
       "  0.14675663988505092,\n",
       "  0.13678530863353183,\n",
       "  0.12647326651428428],\n",
       " 'val_accuracy': [87.28571428571429,\n",
       "  91.46428571428571,\n",
       "  92.66964285714285,\n",
       "  93.70535714285714,\n",
       "  94.51785714285714,\n",
       "  94.97321428571428,\n",
       "  95.33928571428572,\n",
       "  95.80357142857143,\n",
       "  95.77678571428572,\n",
       "  96.24107142857142]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE: train the model using the training function you just implemented.\n",
    "train_model(\n",
    "    model=model_0, \n",
    "    x_train=x_train_tensor, \n",
    "    y_train=y_train_tensor, \n",
    "    x_val=x_val_tensor, \n",
    "    y_val=y_val_tensor, \n",
    "    optimizer=optimizer, \n",
    "    batch_size=batch_size, \n",
    "    loss_func=loss_func, \n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggr_3NzH6faf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Evaluate clean accuracy of the Neural Network using a test set that has not been used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Iyr4Zuo7Gsqz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model into evaluation mode\n",
    "model_0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JItRLnejlXzM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean accuracy of the model is None.\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE: Evaluate model accuracy\n",
    "\n",
    "accuracy = None\n",
    "print(f\"Clean accuracy of the model is {accuracy}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean accuracy of the model is 96.16%.\n"
     ]
    }
   ],
   "source": [
    "# Wrap the test dataset in a DataLoader\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2000, shuffle=False)\n",
    "\n",
    "# Initialize variables for tracking correct predictions and total samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "    for X, y in test_loader:  # Iterate through the test DataLoader\n",
    "        # Move data and labels to the appropriate device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass to compute predictions\n",
    "        preds = model_0(X)\n",
    "\n",
    "        # Get predicted class labels\n",
    "        pred_classes = preds.argmax(dim=1)\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (pred_classes == y).sum().item()\n",
    "\n",
    "        # Track total samples\n",
    "        total += y.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "# Print the clean accuracy\n",
    "print(f\"Clean accuracy of the model is {accuracy:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSkQn-RV6lYF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Implement and execute the PGD attack on 1000 examples of the testing set. The hyperparameters of PGD are given below.\n",
    "The perturbation is bounded by a maximum L-infinity norm, called epsilon (eps), which means that each pixel can be perturbed between -eps and +eps. We initialy set the maximum perturbation to eps = 32/255. For simplicity, you can set the step size alpha = epsilon / 10, and run PGD with only one random restart.\n",
    "\n",
    "You can find the description of PGD in the paper [https://arxiv.org/abs/1706.06083](https://arxiv.org/abs/1706.06083) and an example of another adversarial attack on the PyTorch documentation [https://pytorch.org/tutorials/beginner/fgsm_tutorial.html](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html).\n",
    "Tips: use the F.cross_entropy loss during the attack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "d7BKUQW6tExt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_examples = 1000\n",
    "eps = 32/255\n",
    "n_iter = 50\n",
    "alpha = eps / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "o1fT3GOLn6Gs"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: Generate adversarial examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0O9f_OSnfNk"
   },
   "source": [
    "4. Show the robust accuracy of model_0, that is the accuracy of the model on the adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "chHBAjRin7ss"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: Evaluate model robust accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzvYoy6kAWCE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5. Show the impact of the maximum perturbation allowed (denoted epsilon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lJv7ipEEnyQG"
   },
   "outputs": [],
   "source": [
    "eps = [8/255, 16/255, 32/255, 64/255]\n",
    "alpha = [e/10 for e in eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "pIsJzfm2oJsp"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: compute the adversarial examples for each provided epsilon\n",
    "## (maximum l-infinity norm of the perturbation), and compute the associated robust accuracy\n",
    "## Use a graph to display your result. You may use the [Matplotlib] (https://matplotlib.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhIK7v5vqJWG"
   },
   "source": [
    "6. Using matplotlib, plot 10 adversarial examples, along with their corresponding original images. Choose one original image classified per class (the 10 class should be represented). For each image (adversarial and original), add on the plot the predicted class of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1cDtwHhRqHLs"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ2b1k-uAnNH"
   },
   "source": [
    "**Question**: Please comment your results of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5hkNyHhA2kW"
   },
   "source": [
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIw9leNrGeXW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## 2. Transferability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzMOFr6gG9NZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section we will see how adversarial examples generated on one model can be adversarial on another model using a different architecture.\n",
    "Let suppose a second model which parameters are unknown. For instance, it could be a model deploy on a cloud platform. We will use the examples generated in Section 1 on model_0 to fool this new model denoted model_1.\n",
    "We say that model_0 is a surrogate for model_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc5SBJRMI7s5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Define a neural network architecture for MNIST different than the one used in Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Yyk8EdWJHV8Y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## GIVEN\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roIKZhH_J5uJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Train the neural network model_1 with the same hyperparameters as model_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "jxtPU9qDra7F"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "model_1 = None\n",
    "optimizer = None  # create a new optimizer object when you train a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "jL6wbtlgf_bv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. What is the ratio of successful adversarial examples on model_0 that transfers to model_1 (ie. that are also adversarial for model_1)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "cCCiLpJzsDSz"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## YOUR CODE HERE\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "model_1.eval()\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILXDrrMU_-OO"
   },
   "source": [
    "What do you conclude about the robustness of the model? Can [secrecy](https://en.wikipedia.org/wiki/Security_through_obscurity) defend a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_BJuQISAX3Q"
   },
   "source": [
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gm7YiINCf_bw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Use adversarial training to robustify the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "umy74k6mf_bw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Adversarial training is a common method to robustify models to adversarial examples as described in this paper [https://arxiv.org/abs/1706.06083](https://arxiv.org/abs/1706.06083). In this section you should update the training loop such that 3/4 of the batch is used for training while the remaining forth is first perturbed with PGD and then used for training. You can limit the number of iterations of PDG to 10. Use model_0 architecture from Section 1 in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TfC1zwKlf_bw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Train model_robust using adversarial training. You may want to run it for additional epoch (x2) to reach a similar clean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBzdZhkstpRb"
   },
   "outputs": [],
   "source": [
    "n_iter = 10  # less iterations to accelerate training. But once trained, we will still evaluate the robust accuracy on more iterations for a more powerful attack.\n",
    "eps = 32/255\n",
    "alpha = eps / 5\n",
    "model_robust = Net()  # newly initialized NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeTu-9edtlFo"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "    size = len(dataloader.dataset)\n",
    "    adv_size = int(batch_size/4)\n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader), total=int(size/batch_size)):\n",
    "\n",
    "        # Generate adversarial examples for a forth of the data\n",
    "\n",
    "        model.eval()\n",
    "        ## YOUR CODE HERE\n",
    "        model.train()\n",
    "\n",
    "        # Compute prediction and loss\n",
    "\n",
    "        ## YOUR CODE HERE:\n",
    "\n",
    "        # Backpropagation\n",
    "\n",
    "        ## YOUR CODE HERE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA1T6hFyf_bw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: The rest of training implementation is unchanged.\n",
    "## Do not reuse the same optimizer object!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0tD5Q08Xf_by",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Compare the robust accuracies of model_0 and model_robust using the same hyperparameters of PGD for different eps size, use a graph to show your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxCK8_1quznJ"
   },
   "outputs": [],
   "source": [
    "n_examples = 1000\n",
    "n_iter = 50\n",
    "eps = [8/255, 16/255, 32/255, 64/255]\n",
    "alpha = [e/10 for e in eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvIbjTPmvFUP"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC7D9bT3A5Z7"
   },
   "source": [
    "**Questions**: Please comment your results. Does adversarial training appears to be a valid defense? Please develop threads to validity of the robust accuracy evaluation carried out here. What could be done to improve the evaluation of the robustness of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT5WD9AZBfVE"
   },
   "source": [
    "**ANSWER HERE**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
